{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4bfb95",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“Š Evaluation of PathSlide2Report (Demo Mode)\n",
    "\n",
    "This is a **demo version** of the evaluation notebook.  \n",
    "It comes pre-filled with **synthetic data** so recruiters can immediately see results without downloading TCGA data.\n",
    "\n",
    "---\n",
    "**Modes Available:**\n",
    "- âœ… **Demo Mode (default)** â†’ Uses mock summaries + diagnoses.\n",
    "- ðŸ”¬ **Real Mode** â†’ Runs full evaluation with TCGA data (if available).\n",
    "\n",
    "---\n",
    "Evaluation includes:\n",
    "- BLEU / ROUGE (classic NLP metrics)\n",
    "- Embedding-based similarity to ground truth diagnoses\n",
    "- GPT-as-a-Judge scoring (1â€“5 scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b8ff06",
   "metadata": {},
   "source": [
    "## 1. Demo Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create synthetic logs (as if from Streamlit app)\n",
    "logs = pd.DataFrame([\n",
    "    {\n",
    "        \"metadata\": \"Slide001\",\n",
    "        \"baseline_summary\": \"H&E stained liver tissue with normal architecture.\",\n",
    "        \"rag_summary\": \"Liver tissue H&E at 40x magnification, showing intact histological structure.\"\n",
    "    },\n",
    "    {\n",
    "        \"metadata\": \"Slide002\",\n",
    "        \"baseline_summary\": \"H&E stained colon tissue, features unclear.\",\n",
    "        \"rag_summary\": \"Colon tissue H&E slide showing glandular structures consistent with adenocarcinoma.\"\n",
    "    }\n",
    "])\n",
    "\n",
    "# Create synthetic ground truth metadata\n",
    "df_meta = pd.DataFrame([\n",
    "    {\"slide_id\": \"Slide001\", \"diagnosis\": \"Normal liver tissue\"},\n",
    "    {\"slide_id\": \"Slide002\", \"diagnosis\": \"Colon adenocarcinoma\"}\n",
    "])\n",
    "\n",
    "print(\"Demo logs:\")\n",
    "display(logs)\n",
    "print(\"Demo metadata:\")\n",
    "display(df_meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d755357",
   "metadata": {},
   "source": [
    "## 2. BLEU / ROUGE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65448982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "reference = df_meta[\"diagnosis\"].tolist()\n",
    "generated_baseline = logs[\"baseline_summary\"].tolist()\n",
    "generated_rag = logs[\"rag_summary\"].tolist()\n",
    "\n",
    "def evaluate_summary(reference, candidate):\n",
    "    bleu = sentence_bleu([reference.split()], candidate.split())\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return bleu, scores\n",
    "\n",
    "results = []\n",
    "for ref, base, rag in zip(reference, generated_baseline, generated_rag):\n",
    "    bleu_base, rouge_base = evaluate_summary(ref, base)\n",
    "    bleu_rag, rouge_rag = evaluate_summary(ref, rag)\n",
    "    results.append({\n",
    "        \"Reference\": ref,\n",
    "        \"Baseline\": base,\n",
    "        \"RAG\": rag,\n",
    "        \"BLEU (Baseline)\": bleu_base,\n",
    "        \"BLEU (RAG)\": bleu_rag,\n",
    "        \"ROUGE-L (Baseline)\": rouge_base[\"rougeL\"].fmeasure,\n",
    "        \"ROUGE-L (RAG)\": rouge_rag[\"rougeL\"].fmeasure\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c021adb3",
   "metadata": {},
   "source": [
    "## 3. Embedding-Based Similarity (Demo Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91de5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "baseline_emb = model.encode(logs[\"baseline_summary\"].tolist())\n",
    "rag_emb = model.encode(logs[\"rag_summary\"].tolist())\n",
    "gt_emb = model.encode(df_meta[\"diagnosis\"].tolist())\n",
    "\n",
    "baseline_scores = [cosine_similarity([b],[g])[0][0] for b,g in zip(baseline_emb, gt_emb)]\n",
    "rag_scores = [cosine_similarity([r],[g])[0][0] for r,g in zip(rag_emb, gt_emb)]\n",
    "\n",
    "logs[\"baseline_score\"] = baseline_scores\n",
    "logs[\"rag_score\"] = rag_scores\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([\"Baseline\", \"RAG+Metadata\"], [np.mean(baseline_scores), np.mean(rag_scores)], color=[\"red\",\"green\"])\n",
    "plt.title(\"Average Similarity to Ground Truth (Demo)\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.show()\n",
    "\n",
    "logs[[\"metadata\",\"baseline_score\",\"rag_score\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233886ad",
   "metadata": {},
   "source": [
    "## 4. GPT-as-a-Judge (Demo Mode Stub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0950eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instead of calling GPT (requires API key), we simulate scores\n",
    "# to illustrate what recruiters would see.\n",
    "\n",
    "gpt_eval_df = pd.DataFrame([\n",
    "    {\"slide_id\": \"Slide001\", \"baseline_gpt_score\": 4, \"rag_gpt_score\": 5},\n",
    "    {\"slide_id\": \"Slide002\", \"baseline_gpt_score\": 2, \"rag_gpt_score\": 5}\n",
    "])\n",
    "\n",
    "avg_base = gpt_eval_df[\"baseline_gpt_score\"].mean()\n",
    "avg_rag = gpt_eval_df[\"rag_gpt_score\"].mean()\n",
    "\n",
    "print(\"Average GPT Score (Baseline):\", avg_base)\n",
    "print(\"Average GPT Score (RAG+Metadata):\", avg_rag)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([\"Baseline\", \"RAG+Metadata\"], [avg_base, avg_rag], color=[\"red\",\"green\"])\n",
    "plt.title(\"Average GPT Evaluation Scores (Demo)\")\n",
    "plt.ylabel(\"Score (1â€“5)\")\n",
    "plt.ylim(0,5)\n",
    "plt.show()\n",
    "\n",
    "gpt_eval_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a57d8",
   "metadata": {},
   "source": [
    "## 5. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fc890",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"### ðŸ“Œ Results Summary (Demo Mode)\")\n",
    "print(f\"- Embedding-based similarity: Baseline={np.mean(baseline_scores):.3f}, RAG={np.mean(rag_scores):.3f}\")\n",
    "print(f\"- GPT-as-a-Judge: Baseline={avg_base:.2f}, RAG={avg_rag:.2f}\")\n",
    "print(\"\\nâœ… Overall, RAG+Metadata shows higher alignment with diagnoses compared to Baseline in this demo.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
