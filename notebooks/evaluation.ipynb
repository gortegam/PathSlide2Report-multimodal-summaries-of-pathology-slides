{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "8d9612",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from src.embedder import ClipEmbedder\n",
    "from src.captioner import BlipCaptioner\n",
    "from src.vectorstore import FaissStore\n",
    "from src.rag_inference import build_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "799fda",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df = pd.read_csv(\"../sample_data/metadata.csv\")\n",
    "img = Image.open(\"../sample_data/sample_slide.png\").convert(\"RGB\")\n",
    "\n",
    "# Instantiate models\n",
    "embedder = ClipEmbedder()\n",
    "captioner = BlipCaptioner()\n",
    "\n",
    "# Generate embedding + caption\n",
    "emb = embedder.embed_image(img)\n",
    "caption = captioner.caption(img)\n",
    "\n",
    "print(\"Caption:\", caption)\n",
    "print(\"Embedding shape:\", emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "a89f59",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Fake \"other slides\" (normally you'd load multiple slides)\n",
    "emb2 = emb + np.random.normal(0, 0.01, emb.shape)  # very similar\n",
    "emb3 = np.random.normal(0, 1, emb.shape)           # random slide\n",
    "\n",
    "similarity_matrix = cosine_similarity(np.vstack([emb, emb2, emb3]))\n",
    "print(\"Cosine similarity matrix:\\n\", similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "09b190",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Fake \"reference summaries\"\n",
    "reference_clinical = \"Liver tissue slide, stained with H&E, magnification 40x. Features suggest normal structure.\"\n",
    "generated_clinical = \"H&E stained liver tissue at 40x magnification, features appear normal.\"\n",
    "\n",
    "# BLEU\n",
    "bleu = sentence_bleu([reference_clinical.split()], generated_clinical.split())\n",
    "print(\"BLEU Score:\", bleu)\n",
    "\n",
    "# ROUGE\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference_clinical, generated_clinical)\n",
    "print(\"ROUGE:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "d932d7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "embeddings = np.vstack([emb, emb2, emb3])\n",
    "labels = [\"Slide1\", \"Slide2 (similar)\", \"Slide3 (different)\"]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i, label in enumerate(labels):\n",
    "    plt.scatter(reduced[i,0], reduced[i,1], label=label)\n",
    "plt.legend()\n",
    "plt.title(\"t-SNE of Slide Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "ee1d30",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# 6. Load Streamlit Logs for Evaluation\n",
    "# -----------------------------------------------------\n",
    "log_df = pd.read_csv(\"../logs/run_log.csv\")\n",
    "print(\"Loaded\", len(log_df), \"runs\")\n",
    "\n",
    "# Use the last run for demonstration\n",
    "last = log_df.iloc[-1]\n",
    "\n",
    "reference_summary = \"Liver tissue slide, stained with H&E at 40x magnification. Features suggest normal histological architecture.\"\n",
    "baseline_summary = last[\"baseline_summary\"]\n",
    "rag_summary = last[\"rag_summary\"]\n",
    "\n",
    "# Evaluate both\n",
    "bleu_base, rouge_base = evaluate_summary(reference_summary, baseline_summary)\n",
    "bleu_rag, rouge_rag = evaluate_summary(reference_summary, rag_summary)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"Model\": \"Baseline\", \"BLEU\": bleu_base, \"ROUGE-1\": rouge_base['rouge1'].fmeasure, \"ROUGE-L\": rouge_base['rougeL'].fmeasure},\n",
    "    {\"Model\": \"RAG+Metadata\", \"BLEU\": bleu_rag, \"ROUGE-1\": rouge_rag['rouge1'].fmeasure, \"ROUGE-L\": rouge_rag['rougeL'].fmeasure}\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "af94fa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# 7. Load TCGA Enriched Metadata\n",
    "# -----------------------------------------------------\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"../data/patches_metadata_enriched.csv\"):\n",
    "    print(\"✅ Using enriched metadata (with diagnosis).\")\n",
    "    df_meta = pd.read_csv(\"../data/patches_metadata_enriched.csv\")\n",
    "elif os.path.exists(\"../data/patches_metadata.csv\"):\n",
    "    print(\"⚠️ Using basic metadata (no diagnosis). Run tcga_metadata_fetcher.py for enrichment.\")\n",
    "    df_meta = pd.read_csv(\"../data/patches_metadata.csv\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ No patch metadata found. Run tcga_preprocess.py first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "9a5de5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# 8. Join Streamlit logs with metadata\n",
    "# -----------------------------------------------------\n",
    "logs = pd.read_csv(\"../logs/run_log.csv\")\n",
    "\n",
    "if \"slide_id\" in df_meta.columns:\n",
    "    eval_df = logs.copy()\n",
    "    eval_df[\"slide_id\"] = eval_df[\"metadata\"].astype(str).apply(\n",
    "        lambda x: x.split(\",\")[0] if \"slide_id\" in x else None\n",
    "    )\n",
    "    eval_df = eval_df.merge(df_meta, on=\"slide_id\", how=\"left\")\n",
    "else:\n",
    "    eval_df = logs.copy()\n",
    "    print(\"⚠️ No slide_id found in metadata CSV — only logs will be evaluated.\")\n",
    "\n",
    "print(\"Merged evaluation dataset shape:\", eval_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "1289e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# 9. Embedding-based similarity vs ground truth diagnosis\n",
    "# -----------------------------------------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "if \"diagnosis\" in df_meta.columns:\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    gt_texts = eval_df[\"diagnosis\"].astype(str).tolist()\n",
    "    baseline_emb = model.encode(eval_df[\"baseline_summary\"].astype(str).tolist())\n",
    "    rag_emb = model.encode(eval_df[\"rag_summary\"].astype(str).tolist())\n",
    "    gt_emb = model.encode(gt_texts)\n",
    "\n",
    "    baseline_scores = [cosine_similarity([b], [g])[0][0] for b, g in zip(baseline_emb, gt_emb)]\n",
    "    rag_scores = [cosine_similarity([r], [g])[0][0] for r, g in zip(rag_emb, gt_emb)]\n",
    "\n",
    "    eval_df[\"baseline_score\"] = baseline_scores\n",
    "    eval_df[\"rag_score\"] = rag_scores\n",
    "\n",
    "    print(\"Mean Baseline similarity:\", np.mean(baseline_scores))\n",
    "    print(\"Mean RAG similarity:\", np.mean(rag_scores))\n",
    "else:\n",
    "    print(\"⚠️ No diagnosis column available — cannot compute similarity.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "2eb623",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# 10. Visualization: Baseline vs RAG\n",
    "# -----------------------------------------------------\n",
    "if \"baseline_score\" in eval_df.columns:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar([\"Baseline\", \"RAG+Metadata\"],\n",
    "            [np.mean(eval_df[\"baseline_score\"]), np.mean(eval_df[\"rag_score\"])],\n",
    "            color=[\"red\", \"green\"])\n",
    "    plt.title(\"Average Similarity to Ground Truth Diagnosis\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "b2ed1d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# 11. Qualitative examples\n",
    "# -----------------------------------------------------\n",
    "if \"diagnosis\" in eval_df.columns:\n",
    "    for i, row in eval_df.head(3).iterrows():\n",
    "        print(\"Slide:\", row.get(\"slide_id\", \"unknown\"))\n",
    "        print(\"Ground truth diagnosis:\", row.get(\"diagnosis\", \"N/A\"))\n",
    "        print(\"Baseline summary:\", row[\"baseline_summary\"])\n",
    "        print(\"RAG+Metadata summary:\", row[\"rag_summary\"])\n",
    "        print(\"Baseline similarity:\", row.get(\"baseline_score\", \"N/A\"))\n",
    "        print(\"RAG similarity:\", row.get(\"rag_score\", \"N/A\"))\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "bc4dd3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# 12. GPT-based Evaluation (LLM-as-a-Judge)\n",
    "# -----------------------------------------------------\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai.api_key:\n",
    "    print(\"⚠️ No OpenAI API key found. Set OPENAI_API_KEY to run this block.\")\n",
    "else:\n",
    "    def gpt_score(diagnosis, summary):\n",
    "        \"\"\"\n",
    "        Ask GPT to score how well the summary aligns with the ground truth diagnosis.\n",
    "        Returns a score from 1 (poor alignment) to 5 (excellent alignment).\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are evaluating pathology report summaries.\n",
    "        \n",
    "        Ground truth diagnosis: {diagnosis}\n",
    "        Candidate summary: {summary}\n",
    "\n",
    "        On a scale of 1 to 5:\n",
    "        - 1 = completely wrong or misleading\n",
    "        - 3 = partially correct but incomplete\n",
    "        - 5 = fully correct and well aligned with the diagnosis\n",
    "\n",
    "        Return ONLY the number (1–5).\n",
    "        \"\"\"\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "            max_tokens=3,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        try:\n",
    "            return int(response.choices[0].message.content.strip())\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    # Apply GPT evaluation for first few samples\n",
    "    gpt_results = []\n",
    "    for i, row in eval_df.head(5).iterrows():\n",
    "        if \"diagnosis\" in row and isinstance(row[\"diagnosis\"], str):\n",
    "            base_score = gpt_score(row[\"diagnosis\"], row[\"baseline_summary\"])\n",
    "            rag_score = gpt_score(row[\"diagnosis\"], row[\"rag_summary\"])\n",
    "            gpt_results.append({\n",
    "                \"slide_id\": row.get(\"slide_id\", \"unknown\"),\n",
    "                \"diagnosis\": row[\"diagnosis\"],\n",
    "                \"baseline_summary\": row[\"baseline_summary\"],\n",
    "                \"rag_summary\": row[\"rag_summary\"],\n",
    "                \"baseline_gpt_score\": base_score,\n",
    "                \"rag_gpt_score\": rag_score\n",
    "            })\n",
    "\n",
    "    gpt_eval_df = pd.DataFrame(gpt_results)\n",
    "    display(gpt_eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "659e8c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# 13. Aggregate GPT Scores + Visualization\n",
    "# -----------------------------------------------------\n",
    "if \"baseline_gpt_score\" in gpt_eval_df.columns:\n",
    "    avg_base = gpt_eval_df[\"baseline_gpt_score\"].mean()\n",
    "    avg_rag = gpt_eval_df[\"rag_gpt_score\"].mean()\n",
    "\n",
    "    print(\"Average GPT Score (Baseline):\", avg_base)\n",
    "    print(\"Average GPT Score (RAG+Metadata):\", avg_rag)\n",
    "\n",
    "    # Bar chart comparison\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar([\"Baseline\", \"RAG+Metadata\"], [avg_base, avg_rag], color=[\"red\",\"green\"])\n",
    "    plt.title(\"Average GPT Evaluation Scores\")\n",
    "    plt.ylabel(\"Score (1 = poor, 5 = excellent)\")\n",
    "    plt.ylim(0, 5)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ No GPT scores computed yet. Run the GPT evaluator block first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}