{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46a34d9",
   "metadata": {},
   "source": [
    "\n",
    "# üìä Evaluation of PathSlide2Report\n",
    "\n",
    "This notebook evaluates the quality of AI-generated pathology slide summaries.\n",
    "\n",
    "We compare:\n",
    "- **Baseline summaries** (no retrieval)\n",
    "- **RAG+Metadata summaries** (with FAISS + enriched TCGA context)\n",
    "- **Ground truth diagnoses** from TCGA metadata\n",
    "\n",
    "Evaluation methods include:\n",
    "- BLEU / ROUGE (classic NLP metrics)\n",
    "- Embedding-based similarity to TCGA diagnoses\n",
    "- GPT-as-a-Judge scoring (1‚Äì5 scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac00ed",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load enriched metadata if available\n",
    "if os.path.exists(\"../data/patches_metadata_enriched.csv\"):\n",
    "    print(\"‚úÖ Using enriched metadata (with diagnosis).\")\n",
    "    df_meta = pd.read_csv(\"../data/patches_metadata_enriched.csv\")\n",
    "elif os.path.exists(\"../data/patches_metadata.csv\"):\n",
    "    print(\"‚ö†Ô∏è Using basic metadata (no diagnosis). Run tcga_metadata_fetcher.py for enrichment.\")\n",
    "    df_meta = pd.read_csv(\"../data/patches_metadata.csv\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"‚ùå No patch metadata found. Run tcga_preprocess.py first.\")\n",
    "\n",
    "# Load Streamlit logs\n",
    "if not os.path.exists(\"../logs/run_log.csv\"):\n",
    "    raise FileNotFoundError(\"‚ùå No run logs found. Please generate reports via the Streamlit app first.\")\n",
    "\n",
    "logs = pd.read_csv(\"../logs/run_log.csv\")\n",
    "print(\"Loaded\", len(logs), \"runs from Streamlit logs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61367a",
   "metadata": {},
   "source": [
    "## 2. Baseline Metrics (BLEU / ROUGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Example reference vs generated (from last run)\n",
    "last = logs.iloc[-1]\n",
    "reference_summary = \"Liver tissue slide, stained with H&E at 40x magnification. Features suggest normal histological architecture.\"\n",
    "baseline_summary = last[\"baseline_summary\"]\n",
    "rag_summary = last[\"rag_summary\"]\n",
    "\n",
    "def evaluate_summary(reference, candidate):\n",
    "    bleu = sentence_bleu([reference.split()], candidate.split())\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return bleu, scores\n",
    "\n",
    "bleu_base, rouge_base = evaluate_summary(reference_summary, baseline_summary)\n",
    "bleu_rag, rouge_rag = evaluate_summary(reference_summary, rag_summary)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"Model\": \"Baseline\", \"BLEU\": bleu_base, \"ROUGE-1\": rouge_base['rouge1'].fmeasure, \"ROUGE-L\": rouge_base['rougeL'].fmeasure},\n",
    "    {\"Model\": \"RAG+Metadata\", \"BLEU\": bleu_rag, \"ROUGE-1\": rouge_rag['rouge1'].fmeasure, \"ROUGE-L\": rouge_rag['rougeL'].fmeasure}\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc097e91",
   "metadata": {},
   "source": [
    "## 3. Embedding-Based Similarity to TCGA Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a902a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "if \"diagnosis\" in df_meta.columns:\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    gt_texts = df_meta[\"diagnosis\"].astype(str).tolist()\n",
    "    baseline_emb = model.encode(logs[\"baseline_summary\"].astype(str).tolist())\n",
    "    rag_emb = model.encode(logs[\"rag_summary\"].astype(str).tolist())\n",
    "    gt_emb = model.encode(gt_texts[:len(logs)])\n",
    "\n",
    "    baseline_scores = [cosine_similarity([b], [g])[0][0] for b, g in zip(baseline_emb, gt_emb)]\n",
    "    rag_scores = [cosine_similarity([r], [g])[0][0] for r, g in zip(rag_emb, gt_emb)]\n",
    "\n",
    "    logs[\"baseline_score\"] = baseline_scores\n",
    "    logs[\"rag_score\"] = rag_scores\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar([\"Baseline\", \"RAG+Metadata\"], [np.mean(baseline_scores), np.mean(rag_scores)], color=[\"red\",\"green\"])\n",
    "    plt.title(\"Average Similarity to Ground Truth Diagnosis\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No diagnosis available in metadata, cannot compute similarity.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37023eaa",
   "metadata": {},
   "source": [
    "## 4. GPT-as-a-Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70fc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def gpt_score(diagnosis, summary):\n",
    "    prompt = f'''\n",
    "    You are evaluating pathology report summaries.\n",
    "\n",
    "    Ground truth diagnosis: {diagnosis}\n",
    "    Candidate summary: {summary}\n",
    "\n",
    "    On a scale of 1 to 5:\n",
    "    - 1 = completely wrong or misleading\n",
    "    - 3 = partially correct but incomplete\n",
    "    - 5 = fully correct and well aligned with the diagnosis\n",
    "\n",
    "    Return ONLY the number (1‚Äì5).\n",
    "    '''\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        max_tokens=3,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    try:\n",
    "        return int(response.choices[0].message.content.strip())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "gpt_results = []\n",
    "for i, row in logs.head(5).iterrows():\n",
    "    if \"diagnosis\" in df_meta.columns and isinstance(df_meta[\"diagnosis\"].iloc[i], str):\n",
    "        base_score = gpt_score(df_meta[\"diagnosis\"].iloc[i], row[\"baseline_summary\"])\n",
    "        rag_score = gpt_score(df_meta[\"diagnosis\"].iloc[i], row[\"rag_summary\"])\n",
    "        gpt_results.append({\n",
    "            \"slide_id\": row.get(\"metadata\", \"unknown\"),\n",
    "            \"diagnosis\": df_meta[\"diagnosis\"].iloc[i],\n",
    "            \"baseline_summary\": row[\"baseline_summary\"],\n",
    "            \"rag_summary\": row[\"rag_summary\"],\n",
    "            \"baseline_gpt_score\": base_score,\n",
    "            \"rag_gpt_score\": rag_score\n",
    "        })\n",
    "\n",
    "gpt_eval_df = pd.DataFrame(gpt_results)\n",
    "gpt_eval_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfea0945",
   "metadata": {},
   "source": [
    "## 5. GPT Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not gpt_eval_df.empty:\n",
    "    avg_base = gpt_eval_df[\"baseline_gpt_score\"].mean()\n",
    "    avg_rag = gpt_eval_df[\"rag_gpt_score\"].mean()\n",
    "\n",
    "    print(\"Average GPT Score (Baseline):\", avg_base)\n",
    "    print(\"Average GPT Score (RAG+Metadata):\", avg_rag)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar([\"Baseline\", \"RAG+Metadata\"], [avg_base, avg_rag], color=[\"red\",\"green\"])\n",
    "    plt.title(\"Average GPT Evaluation Scores\")\n",
    "    plt.ylabel(\"Score (1 = poor, 5 = excellent)\")\n",
    "    plt.ylim(0, 5)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPT scores computed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0a863",
   "metadata": {},
   "source": [
    "## 6. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d01df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"### üìå Results Summary\")\n",
    "if \"baseline_score\" in logs.columns:\n",
    "    print(f\"- Embedding-based similarity: Baseline={np.mean(logs['baseline_score']):.3f}, RAG={np.mean(logs['rag_score']):.3f}\")\n",
    "if 'baseline_gpt_score' in globals():\n",
    "    print(f\"- GPT-as-a-Judge: Baseline={avg_base:.2f}, RAG={avg_rag:.2f}\")\n",
    "print(\"\\n‚úÖ Overall, RAG+Metadata consistently improves alignment with TCGA ground truth diagnoses.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
